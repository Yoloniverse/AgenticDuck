
import os
## Langchain libraries
from langchain.llms import Ollama
from langchain.agents import tool, AgentExecutor, create_tool_calling_agent
from langchain_tavily import TavilySearch
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages
from langchain_tavily import TavilySearch
from langchain_ollama import ChatOllama
#from langmem.short_term import SummarizationNode
from langchain_core.messages.utils import count_tokens_approximately
from langgraph.prebuilt import create_react_agent

# from langchain_anthropic import ChatAnthropic
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage, AnyMessage

## LangGraph libraries
from langgraph.checkpoint.memory import InMemorySaver, MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.graph import MessagesState, START, END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.types import Command

## custom made libraries
#from toolings_hr import get_staff_info, get_doc_apprl_info
## other libraries
from pydantic import BaseModel, Field
from typing import Any, TypedDict, Annotated, Literal, List, Dict, Optional
import getpass
import logging.handlers

#sql node
from sqlalchemy import create_engine, text, inspect
# for rag
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
import chromadb
from chromadb.config import Settings
from sentence_transformers import CrossEncoder
from langchain_community.vectorstores import Chroma

######################################################################
#                             Save Log                               #
######################################################################
os.makedirs('./logs', exist_ok=True)
logger = logging.getLogger("Agent")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log_max_size = 1024000
log_file_count = 3
log_fileHandler = logging.handlers.RotatingFileHandler(
        filename=f"./logs/HR_agent.log",
        maxBytes=log_max_size,
        backupCount=log_file_count,
        mode='a')

log_fileHandler.setFormatter(formatter)
logger.addHandler(log_fileHandler)


## Thread (ì„¸ì…˜)ì˜ ëŒ€í™”ë‚´ìš©ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ì²´í¬í¬ì¸í„° . ì—ì´ì „íŠ¸ì— checkpointerë¥¼ ì „ë‹¬í•˜ë©´ì„œ ì—¬ëŸ¬ í˜¸ì¶œ ê°„ ìƒíƒœ ìœ ì§€(short-term memory)
#checkpointer = InMemorySaver()
## long-term memoryë¥¼ ìœ„í•œ ìŠ¤í† ì–´. ëª¨ë“  ìŠ¤ë ˆë“œì—ì„œ ì¬í™œìš©í•  ìˆ˜ ìˆëŠ” ì§€ì‹ì´ í•„ìš”í•  ë•Œ ì”€. ì„œë¹„ìŠ¤ê°€ runningì¤‘ì¼ ë•Œë§Œ ìœ ì§€ë¨. ì˜êµ¬ ì €ì¥ì€ DBë¥¼ ì‚¬ìš©í•´ì•¼ í•¨.
store = InMemoryStore()
## Ollama LLM ê°ì²´ ë§Œë“¤ê¸°
# ollama pul qwen3:8b 
# 4bit ëª¨ë¸: "qwen3:8b-q4_K_M"
# 8bit ëª¨ë¸: "qwen3:8b-q8_0"
# 16bit ëª¨ë¸ : "qwen3:8b-fp16"
llm = ChatOllama(model="qwen3:8b", base_url="http://127.0.0.1:11434", temperature=0.1)

prompt_router = ChatPromptTemplate.from_messages(
    [
        ("system", "ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ì½ê³  ì‚¬ë‚´ ê²°ì¬ ê¸°ì•ˆ ê·œì •ì— ëŒ€í•œ ì§ˆë¬¸ì¸ì§€, ì‚¬ë‚´ ì§ì› ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ëŠ” ì—­í• ì´ì•¼."),
        ("human", "{user_question}"),
        MessagesPlaceholder("messages")
    ]
)

# ë©”ì‹œì§€ë¥¼ ìµœëŒ€ 3ì„¸íŠ¸(6ê°œ)ë¡œ ì œí•œí•˜ëŠ” í•¨ìˆ˜
def add_messages_with_limit(left: list, right: list, max_messages: int = 6) -> list:
    """ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë˜, ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ë¥¼ ì œí•œ"""
    # ê¸°ë³¸ add_messages ë™ì‘ ìˆ˜í–‰
    combined = add_messages(left, right)
    
    # ìµœê·¼ max_messagesê°œì˜ ë©”ì‹œì§€ë§Œ ìœ ì§€ (Human-AI ë²ˆê°ˆì•„ê°€ëŠ” êµ¬ì¡°)
    if len(combined) > max_messages:
        return combined[-max_messages:]
    
    return combined

class AppState(TypedDict, total=False):
    # ëŒ€í™”(í•„ìš”í•˜ë©´ ê³„ì† ëˆ„ì )
    #messages: Annotated[list[dict], add_messages]
    messages: Annotated[list[dict], lambda left, right: add_messages_with_limit(left, right, max_messages=6)]
    # router_nodeì˜ ë¶„ê¸° ê²°ê³¼
    path_results: str
    # for RAG
    rag_retrieved_docs: List[str]
    rag_reranked_docs: List[str]
    rag_check_cnt: int
    rag_error: str
    # for DB search
    sql_db_schema:  str
    sql_draft: str
    sql_result: str
    sql_error_cnt: int
    sql_error_node: str
    sql_error: Optional[str]    
    hallucination_check: bool # sourceë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± í›„ hallucination ê²€í†  
    # ê²€í†  í›„ í†µê³¼í•œ ìµœì¢… ë‹µë³€
    final_answer: str 

# ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
class OutputState(TypedDict):
    # ê²€í†  í›„ í†µê³¼í•œ ìµœì¢… ë‹µë³€
    final_answer: str # ì™¸ë¶€ë¡œ ë°˜í™˜í•˜ëŠ” ë°ì´í„°

class RouteOut(BaseModel):
    route: Literal["policy", "employee", "unclear"]  # ê²°ì¬ê·œì • / ì§ì›ì •ë³´ / ë¶ˆëª…í™•
    confidence: float = Field(ge=0, le=1)

class HallucinationState(TypedDict):
    result: bool

# ë¼ìš°í„° ì²´ì¸
# ì˜¨ë„ 0ìœ¼ë¡œ ê²°ì •ì„± ë†’ì´ê¸°
router_chain = prompt_router | llm.with_config({'temperature': 0}).with_structured_output(RouteOut)

def router_node(state: AppState) -> Command[Literal["rag_init_node", "get_schema", "general"]]:
    '''
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë³´ê³  ë‹¤ìŒ ìŠ¤í… ë¶„ê¸°ë¥¼ ì •í•˜ëŠ” ë…¸ë“œ
    '''
    output = router_chain.invoke({"user_question": state['messages'][-1].content, "messages": state["messages"]})
    # ë¶„ê¸°
    path = output.route if output.confidence >= 0.6 else "unclear"
    if path == "policy":
        return Command(
            goto="rag_init_node",
            update={
                "path_results": path,
            }
        )
    elif path == "employee":
        return Command(
            goto="get_schema",
            update={
                "path_results": path,
            }
        )
    elif path == "unclear":
        return Command(
            goto="general",
            update={
                "path_results": path,
            }
        )

def general(state: AppState) -> AppState:
    prompt_gen = ChatPromptTemplate.from_messages(
    [
        ("system", f"""ë„ˆëŠ” ì‚¬ìš©ìì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Assistantì•¼."""),
        ("human", "{user_question}"),
        MessagesPlaceholder("messages"),
    ])
    gen_chain = prompt_gen | llm.with_config({'temperature': 0.5})
    output = gen_chain.invoke({"user_question": state['messages'][-1].content, "messages": state["messages"]})
    return {
        "messages": AIMessage(content=output.content.split('</think>\n\n')[-1]),
        "final_answer": output.content.split('</think>\n\n')[-1]
        }

class SQLManager:
    def __init__(self):
        ## db ì—°ê²°
        self.db_user = "admin"
        self.db_password = "sdt251327"
        self.db_host = "127.0.0.1"
        self.db_name = "langgraph" 
        self.connection_string = f"mysql+pymysql://{self.db_user}:{self.db_password}@{self.db_host}/{self.db_name}"
        self.engine = create_engine(self.connection_string)
        self.connection = self.engine.connect()
        self.inspector = inspect(self.engine)


    def get_schema(self, state: AppState) -> AppState:
        try:
            db_structure= """"""
            # ìŠ¤í‚¤ë§ˆ ìˆœíšŒ
            for schema_name in self.inspector.get_schema_names():
                if schema_name == db_name:
                    for idx, table_name in enumerate(self.inspector.get_table_names(schema=schema_name)):
                        # ì»¬ëŸ¼ ì´ë¦„ê³¼ íƒ€ì… ìˆ˜ì§‘
                        columns = self.inspector.get_columns(table_name, schema=schema_name)
                        column_list = [
                            f"{col['name']}"
                            for col in columns
                        ]
                        db_structure += f'\n[DB ìŠ¤í‚¤ë§ˆ]\n{idx}.table_name: {table_name}\n{idx}.columns: {column_list}'
            return {
                "sql_db_schema": db_structure
            }
        except Exception as e:
            return {
            "sql_error_node": "get_schema",
            "sql_error": e
            }

    def sql_gen_node(self, state: AppState) -> Command[Literal["sql_execute_node", END]]:
        try:
            # error ë©”ì„¸ì§€ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì„œ ìƒì„±
            sql_error = state["sql_error"]
            sql_error_cnt = state["sql_error_cnt"]
            print(f"sql_error_cnt: {sql_error_cnt}")

            if sql_error == None:
                prompt_sql = ChatPromptTemplate.from_messages(
                [
                    ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì–»ê¸° ìœ„í•´ ì•„ë˜ [DB ìŠ¤í‚¤ë§ˆ]êµ¬ì¡°ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–»ê¸°ìœ„í•œ SQL ë¬¸ì„ ë§Œë“œëŠ” ì—­í• ì´ì•¼. 
                                ê°€ì¥ ë‹µë³€ì„ ì˜ ì´ëŒì–´ ë‚¼ ìˆ˜ìˆëŠ” SQL ì¡°ê±´ì´ ë¬´ì—‡ì¼ì§€ step by stepìœ¼ë¡œ ì¶©ë¶„íˆ ìƒê°í•´. ì ˆëŒ€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ë¶™ì´ì§€ ë§ê³ , SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´.
                                \n[DB ìŠ¤í‚¤ë§ˆ]\n
                                {state['sql_db_schema']}"""),
                    ("human", "{user_question}"),
                ])
                sql_chain = prompt_sql | llm.with_config({'temperature': 0})
                output = sql_chain.invoke({"user_question": state['messages'][-1].content})
                return Command(
                    goto="sql_execute_node",
                    update={
                        "sql_draft": output.content.split('</think>\n\n')[-1]
                    }
                )

            # ì—ëŸ¬ ë°œìƒí•˜ì—¬ ìµœëŒ€ 3ë²ˆ ë‹¤ì‹œ ì‹œë„
            elif sql_error != None and sql_error_cnt < 4:
                print(f"sql_draft: {state['sql_draft']}")
                prompt_sql = ChatPromptTemplate.from_messages(
                [
                    ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì–»ê¸° ìœ„í•´ ì•„ë˜ [DB ìŠ¤í‚¤ë§ˆ]êµ¬ì¡°ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–»ê¸°ìœ„í•œ SQL ë¬¸ì„ ë§Œë“œëŠ” ì—­í• ì´ì•¼. 
                                ë„ˆëŠ” ë°©ê¸ˆ ì˜ëª»ëœ SQLì„ ë§Œë“¤ì–´ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´. ì•„ë˜ ì‚¬í•­ë“¤ ì°¸ê³ í•´ì„œ ì˜¬ë°”ë¥¸ SQLë¬¸ì„ ë‹¤ì‹œ ë§Œë“¤ì–´.
                                ì ˆëŒ€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ë¶™ì´ì§€ ë§ê³ , SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´.
                                
                                [ì¤‘ìš”í•œ ì œì•½ì‚¬í•­]
                                - ì‚¬ìš©ì ì§ˆë¬¸: {state['messages'][-1].content}
                                - ì´ì „ì— ì‹¤íŒ¨í•œ SQL: {state['sql_draft']}
                                - ë°œìƒí•œ ì—ëŸ¬: {state['sql_error']}
                                - ì´ë²ˆì€ {sql_error_cnt}ë²ˆì§¸ ì‹œë„ì…ë‹ˆë‹¤.
                                - ì ˆëŒ€ ì´ì „ì— ì‹¤íŒ¨í•œ SQLê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , ì™„ì „íˆ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´.
                                - SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´.

                                [DB ìŠ¤í‚¤ë§ˆ]
                                {state['sql_db_schema']}"""),
                    ("human", "{user_question}"),
                ])
                sql_chain = prompt_sql | llm.with_config({'temperature': 0.1 * sql_error_cnt})
                output = sql_chain.invoke({"user_question": state['messages'][-1].content}, config={"timeout": 30})
                return Command(
                    goto="sql_execute_node",
                    update={
                        "sql_draft": output.content.split('</think>\n\n')[-1]
                    }
                )

            elif sql_error != None and sql_error_cnt == 4:
                return Command(
                    goto=END,
                    update={
                        "final_answer": "ë°ì´í„° ê²€ìƒ‰ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹­ì‹œì˜¤.",
                    }
                )

        except Exception as e:
            return {
                "sql_error_node": "sql_gen_node",
                "sql_error": e
            }

    def sql_execute_node(self, state: AppState) -> Command[Literal["sql_gen_node", "sql_final_answer_gen"]]:
        sql_draft = state["sql_draft"]
        try:
            with engine.connect() as self.connection:
                result = self.connection.execute(text(sql_draft))
                rows = result.fetchall() 
                # for row in result:
                #     print(row)
            if not rows or None in rows:
                raise Exception("EMPTY_RESULT") 
                
            # ë¹ˆ ê²°ê³¼ë„ ì—ëŸ¬ë¡œ ì²˜ë¦¬ 
            else:
                return Command(
                goto="sql_final_answer_gen",
                update={
                    "sql_result": rows,
                    "sql_error": None,
                    }
                )

        # ì‹¤í–‰ ì—ëŸ¬ ë°œìƒ ì‹œ sql_gen_node ë…¸ë“œë¡œ ëŒì•„ê°€ë©°, error ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê³  error countë¥¼ ì˜¬ë¦¼.
        except Exception as e:
            return Command(
                goto="sql_gen_node",
                update={
                    "sql_error_node": "sql_execute_node",
                    "sql_error": str(e),
                    "sql_error_cnt": state['sql_error_cnt'] + 1
                    }
                )

    def sql_final_answer_gen(self,state: AppState) -> AppState:
        sql_result = state['sql_result']
        sql_draft = state['sql_draft']
        prompt_final = ChatPromptTemplate.from_messages(
        [
            ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—­í• ì´ì•¼. ì•„ë˜ [ì •ë³´]ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ [SQLë¬¸]ìœ¼ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ DBì—ì„œ ì¡°íšŒí•œ ê²°ê³¼ì•¼. ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì„ ë§ë¶™ì´ê±°ë‚˜ ë³€í˜•í•˜ì—¬ í™˜ê°ì„ ì¼ìœ¼í‚¤ì§€ ë§ˆ.
                        \n[SQLë¬¸]\n {sql_draft}
                        \n[ì •ë³´]\n {sql_result}"""),
            ("human", "{user_question}"),
            MessagesPlaceholder("messages")
        ])
        final_chain = prompt_final | llm.with_config({'temperature': 0})
        output = final_chain.invoke({"user_question": state['messages'][-1].content, "messages": state["messages"]})
        return {
            "messages": AIMessage(content=output.content.split('</think>\n\n')[-1]),
            "final_answer": output.content.split('</think>\n\n')[-1]
        }

class RAGManager:
    def __init__(self):
        # chromadb ìƒìˆ˜ ì •ì˜
        self.CHROMA_DIR = "./chroma_db"
        self.EMBEDDING_MODEL = "BAAI/bge-m3"  # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
        self.RERANK_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Rerank ëª¨ë¸
        self.COLLECTION = "approval_guide" 
        self.top_k = 3 # í”„ë¡¬í”„íŠ¸ì— ì œê³µí•  ë¬¸ì„œ ê°¯ìˆ˜
        self.embeddings = None
        self.reranker = None

    def rag_init_node(self, state: AppState) -> Command[Literal["rag_execute_node", END]]:
        try:
            if self.embeddings and self.reranker:
                pass
            else:
                self.embeddings = HuggingFaceEmbeddings(
                        model_name=self.EMBEDDING_MODEL,
                        model_kwargs={'device': 'cuda:0'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                self.reranker = CrossEncoder(self.RERANK_MODEL)

            # ê¸°ì¡´ DBê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if os.path.exists(self.CHROMA_DIR):
                self.vectorstore = Chroma(
                    collection_name=self.COLLECTION,
                    persist_directory=self.CHROMA_DIR,
                    embedding_function=self.embeddings
                )

                return Command(
                    goto="rag_execute_node",
                    update={
                        "rag_error": None,
                    }
                )
            else:
                return Command(
                    goto=END,
                    update={
                        "rag_error": "vectorestore initialize ì‹¤íŒ¨",
                    }
                )
        except Exception as e:
            print(f"error in initialize_RAG_comp: {e}")
            return Command(
                    goto=END,
                    update={
                        "rag_error": "initialize_RAG_component ì‹¤íŒ¨",
                    }
                )
            

    def rag_execute_node(self, state: AppState) -> AppState:
        # retrive
        self.retriever = self.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
        documents = self.retriever.get_relevant_documents(state['messages'][-1].content)
        # rerank
        query_doc_pairs = [(state['messages'][-1].content, doc.page_content) for doc in documents]
        scores = self.reranker.predict(query_doc_pairs)
        scored_docs = list(zip(scores, documents))
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        # ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜
        reranked_docs = ['<Document>' + doc.page_content[:] + '</Document>' for score, doc in scored_docs[:self.top_k]]

        return {
            "rag_retrieved_docs": documents,
            "rag_reranked_docs": reranked_docs
        }

    def rag_final_answer_gen(self, state: AppState) -> AppState:
        rag_reranked_docs = state['rag_reranked_docs']

        prompt_final = ChatPromptTemplate.from_messages(
        [
            ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—­í• ì´ì•¼. 
                        \n[ì¡°ê±´]\n ì•„ë˜ [ì •ë³´]ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ vectorDBì—ì„œ ì¡°íšŒí•œ ê²°ê³¼ ë¬¸ì„œì•¼. ëª¨ë¥´ëŠ” ë¶€ë¶„ì€ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ê³ , ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì„ ë§ë¶™ì´ê±°ë‚˜ ë³€í˜•í•˜ì—¬ í™˜ê°ì„ ì¼ìœ¼í‚¤ì§€ ë§ˆ.
                        \n[ì •ë³´]\n {rag_reranked_docs}"""),
            ("human", "{user_question}"),
            MessagesPlaceholder("messages")
        ])
        final_chain = prompt_final | llm.with_config({'temperature': 0})
        output = final_chain.invoke({"user_question": state['messages'][-1].content, "messages": state["messages"]})
        return {
            "messages": AIMessage(content=output.content.split('</think>\n\n')[-1]),
            "final_answer": output.content.split('</think>\n\n')[-1]
        }

    def hallucination_check(self, state: AppState) -> AppState:
        rag_reranked_docs = state['rag_reranked_docs']
        final_answer = state['final_answer']

        prompt_check = ChatPromptTemplate.from_messages(
        [
            ("system", f"""ë„ˆëŠ” RAG(Retrieval-Augmented Generation)ê²°ê³¼ë¬¼ì¸ [ë¬¸ì„œ ì •ë³´]ê³¼ LLM ëª¨ë¸ì´ ìƒì„±í•œ [ìƒì„± ë‹µë³€]ì„ ë¹„êµí•˜ì—¬, [ìƒì„± ë‹µë³€]ì— [ë¬¸ì„œ ì •ë³´]ì— ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ìˆëŠ”ì§€ hallucination ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ì—­í• ì´ì•¼.
                        \n[ì¡°ê±´]\n : hallucination ë°œìƒ ì‹œ True, ì—†ì„ ì‹œ Falseë¥¼ ë°˜í™˜í•˜ì‹œì˜¤. ë‹¤ë¥¸ ë¬¸ì¥ ì—†ì´ ë¬´ì¡°ê±´ True ë˜ëŠ” Falseë¡œë§Œ ë‹µí•˜ì„¸ìš”.
                        \n[ë¬¸ì„œ ì •ë³´]\n {rag_reranked_docs}
                        \n[ìƒì„± ë‹µë³€]\n {final_answer}
                        """),
            ("human", "{user_question}")
        ])
        final_chain = prompt_check | llm.with_config({'temperature': 0}).with_structured_output(HallucinationState)
        output = final_chain.invoke({"user_question": state['messages'][-1].content})
        check_result = output['result']

        # hallucination ë°œìƒ
        if check_result:
            return {
                        "hallucination_check": check_result,
                        "final_answer": state["final_answer"] + "\n* ì´ ë‹µë³€ì€ ì •í™•í•˜ì§€ ì•Šì€ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ì  ì°¸ê³ ë°”ëë‹ˆë‹¤.\n"
                    }
        # hallucination ë°œìƒí•˜ì§€ ì•Šì•„ì„œ ì¢…ë£Œ
        else:
            return {
                        "hallucination_check": check_result,
                    }

rag_manager = RAGManager()
sql_manager = SQLManager()

builder = StateGraph(AppState)
# node
builder.add_node("router", router_node)
builder.add_node("general", general)
builder.add_node("get_schema", sql_manager.get_schema)
builder.add_node("sql_gen_node", sql_manager.sql_gen_node)
builder.add_node("sql_execute_node", sql_manager.sql_execute_node)
builder.add_node("sql_final_answer_gen", sql_manager.sql_final_answer_gen)
builder.add_node("rag_init_node", rag_manager.rag_init_node)
builder.add_node("rag_execute_node", rag_manager.rag_execute_node)
builder.add_node("rag_final_answer_gen", rag_manager.rag_final_answer_gen)
builder.add_node("hallucination_check", rag_manager.hallucination_check)
# builder.add_conditional_edges(
#     "router",
#     router_node,
#     {
#         "policy": "rag_node",
#         "employee": "sql_gen_node",
#         "unclear": "general"
#     },
# )
builder.add_edge(START, "router")
builder.add_edge("get_schema", "sql_gen_node")
builder.add_edge("sql_final_answer_gen", END)
builder.add_edge("general", END)
builder.add_edge("rag_execute_node", "rag_final_answer_gen")
builder.add_edge("rag_final_answer_gen", "hallucination_check")
builder.add_edge("hallucination_check", END)

checkpointer = MemorySaver()

#compile
graph = builder.compile(checkpointer=checkpointer)

"""
######## streamìœ¼ë¡œ ì‹¤í–‰ ########
# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id": random_uuid()})
# ì§ˆë¬¸ ì…ë ¥
inputs = {"messages": [{"role":"user","content":"ì™¸ê·¼ êµí†µë¹„ ì²­êµ¬ë°©ë²• ì•Œë ¤ì¤˜"}], "sql_error": None, "rag_error": None, "sql_error_cnt": 0, "rag_check_cnt":0, "sql_error_node": "none"}
# ê·¸ë˜í”„ ì‹¤í–‰
#print(graph.get_graph().nodes.keys())
prev_node = ""
node_names=['rag_final_answer_gen','hallucination_check']
for chunk_msg, metadata in graph.stream(inputs, config, stream_mode="messages"):
    curr_node = metadata["langgraph_node"]
    if not node_names or curr_node in node_names:
        # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
        if curr_node != prev_node:
            print("\n" + "=" * 50)
            print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
            print("- " * 25)
        print(chunk_msg.content, end="", flush=True)

        prev_node = curr_node
"""

# ì‹¤í–‰ ì˜ˆì‹œ
# out_1 = graph.invoke(
#     {"messages": [{"role":"user","content":"ê¹€ë‹¤ì—°ì€ ì–´ëŠ ë¶€ì„œ, ì–´ëŠ íŒ€ ì‚¬ì›ì´ë‹ˆ?"}], "sql_error": None, "sql_error_cnt": 0, "sql_error_node": "none"},
#     {"configurable": {"thread_id": "t1"}}
# )
# print(out_1)
# print(out_1['sql_draft'])
# print(out_1['sql_result'])
# print(out_1['final_answer'])

# # ì‹¤í–‰ ì˜ˆì‹œ
out = graph.invoke(
    {"messages": [{"role":"user","content":"QX ê°œë°œì‹¤ì—ì„œ ì¼í•˜ëŠ” ì‚¬ëŒ ì´ë¦„ì„ ë‹¤ ì•Œë ¤ì¤˜"}], "sql_error": None, "sql_error_cnt": 0, "sql_error_node": "none"},
    {"configurable": {"thread_id": "t1"}}
)
print(out['final_answer'])

# ì‹¤í–‰ ì˜ˆì‹œ RAG
out_2 = graph.invoke(
    {"messages": [{"role":"user","content":"ì™¸ê·¼ êµí†µë¹„ ì²­êµ¬ë°©ë²• ì•Œë ¤ì¤˜"}], "sql_error": None, "rag_error": None, "sql_error_cnt": 0, "rag_check_cnt":0, "sql_error_node": "none"},
    {"configurable": {"thread_id": {"thread_id": "t1"}}}
)
#print(out_2)
print(out_2['final_answer'])


# ì‹¤í–‰ ì˜ˆì‹œ RAG
out_3 = graph.invoke(
    {"messages": [{"role":"user","content":"ë‚˜ëŠ” ì˜ì¹´ ì•ˆì¼ëŠ”ë°, ì™¸ê·¼ êµí†µë¹„ ì²­êµ¬ ì–´ë–»ê²Œ í•˜ë‹ˆ?"}], "sql_error": None, "rag_error": None, "sql_error_cnt": 0, "rag_check_cnt":0, "sql_error_node": "none"},
    {"configurable": {"thread_id": "t1"}}
)
print(out_3['final_answer'])


#graph visualization
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception as e :
    # This requires some extra dependencies and is optional
    print(e)
    pass

# from langgraph.graph import draw_mermaid_png, MermaidDrawMethod

# # ë¡œì»¬ ë¸Œë¼ìš°ì €ë¡œ ë Œë”ë§
# draw_mermaid_png(
#     graph, 
#     draw_method=MermaidDrawMethod.PYPPETEER  # ë¡œì»¬ ë Œë”ë§
# )


# from langgraph.graph import MermaidDrawMethod

# png = graph.get_graph().draw_mermaid_png(
#     draw_method=MermaidDrawMethod.PYPPETEER,  # ì›ê²©(API) ì•ˆ ì“°ê³  ë¡œì»¬ì—ì„œ ë Œë”
#     max_retries=5, retry_delay=2.0
# )


# from IPython.display import Image, display
# from langchain_core.runnables.graph import  MermaidDrawMethod
# display(Image(graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER,)))


#################333 teddynote stream ###############
# from langchain_teddynote.messages import stream_graph, random_uuid
# from langchain_core.runnables import RunnableConfig

# # config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
# config = RunnableConfig(recursion_limit=20, configurable={"thread_id": random_uuid()})

# # ì§ˆë¬¸ ì…ë ¥
# inputs = {"messages": [{"role":"user","content":"ì™¸ê·¼ êµí†µë¹„ ì²­êµ¬ë°©ë²• ì•Œë ¤ì¤˜"}], "sql_error": None, "rag_error": None, "sql_error_cnt": 0, "rag_check_cnt":0, "sql_error_node": "none"}


# # ê·¸ë˜í”„ ì‹¤í–‰
# stream_graph(graph, inputs, config,["rag_final_answer_gen", "hallucination_check"])

