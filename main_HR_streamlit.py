import time
import streamlit as st
import uuid
import os
import sys
import traceback
## Langchain libraries
#from langchain.llms import Ollama
from langchain.agents import tool, AgentExecutor, create_tool_calling_agent
#from langchain_tavily import TavilySearch
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
#from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages
from langchain_ollama import ChatOllama
#from langmem.short_term import SummarizationNode
#from langchain_core.messages.utils import count_tokens_approximately
#from langgraph.prebuilt import create_react_agent

# from langchain_anthropic import ChatAnthropic
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage, AnyMessage

## LangGraph libraries
from langgraph.checkpoint.memory import InMemorySaver, MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.graph import MessagesState, START, END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.types import Command

## other libraries
from pydantic import BaseModel, Field
from typing import Any, TypedDict, Annotated, Literal, List, Dict, Optional
import getpass
import logging.handlers

#sql node
from sqlalchemy import create_engine, text, inspect
# for rag
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
import chromadb
from chromadb.config import Settings
from sentence_transformers import CrossEncoder
from langchain_community.vectorstores import Chroma
# tools
sys.path.append("/home/sdt/Workspace/dykim/Langraph/AgenticDuck")
from toolings import taviliy_web_search_tool 

######################################################################
#                             Save Log                               #
######################################################################
os.makedirs('./logs', exist_ok=True)
logger = logging.getLogger("Agent")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log_max_size = 1024000
log_file_count = 3
log_fileHandler = logging.handlers.RotatingFileHandler(
        filename=f"./logs/HR_agent.log",
        maxBytes=log_max_size,
        backupCount=log_file_count,
        mode='a')

log_fileHandler.setFormatter(formatter)
logger.handlers.clear()
logger.addHandler(log_fileHandler)
logger.propagate = False


# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="HR ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="centered",
    initial_sidebar_state="auto"
)
st.title("ğŸ¤– HR ì±—ë´‡")
st.markdown("ì‚¬ë‚´ ê²°ì¬ ê·œì •ê³¼ ì§ì› ì •ë³´ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”!")
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())

# "qwen3:8b-fp16" / "qwen3:8b"
@st.cache_resource
def initialize_graph():
    logger.info("ì±„íŒ… ì´ˆê¸°í™”")
    llm = ChatOllama(model="qwen3:8b-fp16", base_url="http://127.0.0.1:11434")

    # ë©”ì‹œì§€ë¥¼ ìµœëŒ€ 3ì„¸íŠ¸(6ê°œ)ë¡œ ì œí•œí•˜ëŠ” í•¨ìˆ˜
    def add_messages_with_limit(left: list, right: list, max_messages: int = 6) -> list:
        """ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ë˜, ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ë¥¼ ì œí•œ"""
        # ê¸°ë³¸ add_messages ë™ì‘ ìˆ˜í–‰
        combined = add_messages(left, right)
        
        # ìµœê·¼ max_messagesê°œì˜ ë©”ì‹œì§€ë§Œ ìœ ì§€ (Human-AI ë²ˆê°ˆì•„ê°€ëŠ” êµ¬ì¡°)
        if len(combined) > max_messages:
            return combined[-max_messages:]
        
        return combined

    class AppState(TypedDict, total=False):
        # ëŒ€í™”(í•„ìš”í•˜ë©´ ê³„ì† ëˆ„ì )
        messages: Annotated[list[dict], lambda left, right: add_messages_with_limit(left, right, max_messages=6)]
        query: str
        # router_nodeì˜ ë¶„ê¸° ê²°ê³¼
        path_results: str
        web_results : List[str]
        query_rewrite: Optional[bool]
        # for RAG
        rag_retrieved_docs: List[str]
        rag_reranked_docs: List[str]
        rag_check_cnt: int
        rag_error: str
        # for DB search
        sql_db_schema: str
        sql_draft: str
        sql_result: str
        sql_error_cnt: int
        sql_error_node: str
        sql_error: Optional[str] 
        # sourceë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± í›„ hallucination ê²€í†     
        hallucination_check: dict 
        # ê²€í†  í›„ í†µê³¼í•œ ìµœì¢… ë‹µë³€
        final_answer: str 
        # ì‚¬ìš©ëœ ë„êµ¬ ì´ë¦„ë“¤
        tools_used: Optional[List[str]]  
        tool_calls_made: Optional[bool]

    # ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
    class OutputState(TypedDict):
        # ê²€í†  í›„ í†µê³¼í•œ ìµœì¢… ë‹µë³€
        final_answer: str # ì™¸ë¶€ë¡œ ë°˜í™˜í•˜ëŠ” ë°ì´í„°


    class RouteOut(BaseModel):
        route: Literal["policy", "employee", "general"]  # ê²°ì¬ê·œì • / ì§ì›ì •ë³´ / ë¶ˆëª…í™•
        confidence: float = Field(ge=0, le=1)

    class HallucinationState(TypedDict):
        result: bool
        reason: str
    class SearchState(TypedDict):
        result: bool


    # ë¼ìš°í„° ì²´ì¸
    # ì˜¨ë„ 0ìœ¼ë¡œ ê²°ì •ì„± ë†’ì´ê¸°
    prompt_router = ChatPromptTemplate.from_messages(
        [
            ("system", """ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ì½ê³  ë¶„ì„í•˜ì—¬ ëª©ì ì— ë§ê²Œ ë¶„ë¥˜í•˜ëŠ” ì—­í• ì´ì•¼.
                        [ì¤‘ìš” ì›ì¹™]
                            - ì§ˆë¬¸ìëŠ” íšŒì‚¬ ì§ì›ì´ê¸° ë•Œë¬¸ì— ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ë‚´ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•œë‹¤ëŠ” ê²ƒì„ ëª…ì‹¬í•´.
                            - ì‚¬ë‚´ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì¸ë° ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë¼ê³  ì°©ê°í•˜ì§€ ì•Šë„ë¡ ì¶©ë¶„íˆ ìƒê°í•´.
                            - ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í†µí•´ ì–»ê³ ì‹¶ì€ ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ë¥¼ í•µì‹¬ìœ¼ë¡œ ì¤‘ìš”í•˜ê²Œ ìƒê°í•´.
                        [ì¡°ê±´]
                            - "policy" = ì‚¬ë‚´ ì¸ì‚¬ ê·œì •, ê·¼ë¬´ ê·œì •, ì—…ë¬´ í”„ë¡œì„¸ìŠ¤, ê²°ì¬/ê¸°ì•ˆ ì‘ì„± ê·œì •, ì¶œì¥ë¹„Â·ê²½ë¹„ ì²˜ë¦¬, íœ´ê°€Â·ê·¼íƒœ, ë³´ê³ ì„œ ì œì¶œ, ì „ê²° ê·œì • ë“± íšŒì‚¬ ê·œì •ì´ë‚˜ ì œë„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸  
                            - "employee" = ì‚¬ë‚´ ì§ì› ê°œì¸ì˜ ì´ë¦„, ì—°ë½ì²˜, ë¶€ì„œ, ì§ê¸‰, ë‹´ë‹¹ ì—…ë¬´ ë“± ì¸ì‚¬/ì¡°ì§ ì •ë³´ ê´€ë ¨ ì§ˆë¬¸
                            - "general" = "policy", "employee" ë²”ì£¼ì— í•´ë‹¹ë˜ì§€ ì•Šê³ , ì‚¬ë‚´ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì§€ ì•Šê³  ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸
                            - "answer_unavailable" = ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ 1~4ë²ˆì— í•´ë‹¹ë˜ëŠ” ì§ˆë¬¸
                                1. ì‹œìŠ¤í…œ ë‚´ë¶€ ì •ë³´: ì„œë²„ ì£¼ì†Œ, ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ì •ë³´, API í‚¤, í† í°, ë¹„ë°€ë²ˆí˜¸ ë“± ì‹œìŠ¤í…œ ë³´ì•ˆê³¼ ê´€ë ¨ëœ ì •ë³´. ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°, ë¡œê·¸, ì†ŒìŠ¤ì½”ë“œ, ëª¨ë¸ íŒŒë¼ë¯¸í„°, ìš´ì˜ ì¸í”„ë¼ ì„¸ë¶€ì‚¬í•­
                                2. ê°œì¸ì •ë³´ ë° ë¯¼ê° ë°ì´í„°: ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ê³„ì¢Œë²ˆí˜¸, ê¸‰ì—¬ ë‚´ì—­, ì¸ì‚¬í‰ê°€, ì±„ìš© ì‹¬ì‚¬ ê²°ê³¼ ë“± ë¯¼ê°í•œ ê°œì¸ ì‹ ìƒ ì •ë³´. íŠ¹ì • ì§ì›ì˜ ì‚¬ì ì¸ ìƒí™œ, ê°œì¸ ê¸°ë¡, ë¹„ê³µê°œ ê±´ê°•Â·ì¬ë¬´ ì •ë³´
                                3. ë³´ì•ˆ/ì •ì±…ìƒ ì œê³µ ë¶ˆê°€í•œ ìš”ì²­: íšŒì‚¬ì˜ ë³´ì•ˆ ê·œì •, ë¯¸ê³µê°œ ì‚¬ì—… ì „ëµ, ê³„ì•½ ë‚´ìš©, ë²•ì  ë¶„ìŸ ìë£Œ. ê³µê°œê°€ ê¸ˆì§€ëœ ê¸°ë°€ë¬¸ì„œë‚˜ ë‚´ë¶€ ë¬¸ê±´ ìš”ì²­
                                4. ê¸°íƒ€: ì™¸ë¶€ ê³µê°œê°€ í—ˆìš©ë˜ì§€ ì•Šì€ ë‚´ë¶€ ì‹œìŠ¤í…œ ë°ì´í„° ë° ë¡œê·¸.
                            """),
            ("human", "{user_question}"),
        ]
    )
    router_chain = prompt_router | llm.with_config({'temperature': 0.1}).with_structured_output(RouteOut)

    def router_node(state: AppState) -> Command[Literal["rag_init_node", "get_schema", "general", "security_filter"]]:
        '''
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë³´ê³  ë‹¤ìŒ ìŠ¤í… ë¶„ê¸°ë¥¼ ì •í•˜ëŠ” ë…¸ë“œ
        '''
        logger.info(' == [router_node] node init == ')
        
        output = router_chain.invoke({
            "user_question": state['messages'][-1].content
            })
        # ë¶„ê¸°
        #path = output.route if output.confidence >= 0.6 else "general"
        path = output.route
        logger.info(f"path : {path}")
        if path == "policy":
            return Command(
                goto="rag_init_node",
                update={
                    "path_results": path,
                    "query": state['messages'][-1].content
                }
            )
        elif path == "employee":
            return Command(
                goto="get_schema",
                update={
                    "path_results": path,
                    "query": state['messages'][-1].content
                }
            )
        elif path == "general":
            return Command(
                goto="general",
                update={
                    "path_results": path,
                    "query": state['messages'][-1].content
                }
            )
        elif path == "security_filter":
            return Command(
                goto="general",
                update={
                    "path_results": path,
                    "query": state['messages'][-1].content
                }
            )

    tools = [taviliy_web_search_tool]
    prompt_gen = ChatPromptTemplate.from_messages(
        [
            ("system", f"""ë„ˆëŠ” ì‚¬ìš©ìì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Assistantì•¼. ë§Œì•½ ì›¹ ì„œì¹˜ê°€ í•„ìš”í•œ ì§ˆë¬¸ì´ë¼ë©´ ì œê³µëœ web search toolì„ ì‚¬ìš©í•˜ê³ , ì•„ë‹Œ ê²½ìš° ë°”ë¡œ ë‹µë³€ì„ ìƒì„±í•´.
                        [web searchê°€ í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œ]
                         - ì‹¤ì‹œê°„ ë°ì´í„°ì— ì ‘ê·¼í•´ì•¼ í•˜ëŠ” ê²½ìš°
                         - ìµœì‹  ë‰´ìŠ¤, ì£¼ì‹ ê°€ê²©, ë‚ ì”¨, ë‚ ì§œ
                         - LLMì´ í•™ìŠµí•˜ì§€ ì•Šì•„ ê²€ìƒ‰ì´ í•„ìš”í•œ ë°ì´í„°
                        """),
            ("human", "{user_question}"),
            MessagesPlaceholder("messages"),
            ("placeholder", "{agent_scratchpad}"),
        ])
    agent = create_tool_calling_agent(llm.with_config({'temperature': 0.3}), tools, prompt_gen)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, return_intermediate_steps=True) 
    
    def general(state: AppState) -> Command[Literal["query_rewrite", END]]:
        logger.info(' == [general] node init == ')
        #output = agent_executor.invoke({"user_question": "ì˜¤ëŠ˜ í•œêµ­ì€ ë©°ì¹ ì´ì•¼?", "messages": ["messages"]})
        output = agent_executor.invoke({"user_question": state['query'], "messages": state["messages"]})
        tools_used = []
        search_contents = []
        if 'intermediate_steps' in output and output['intermediate_steps']:
            tool_calls_made = True
            for step in output['intermediate_steps']:
                if len(step) >= 2:
                    action, result = step[0], step[1]
                    if hasattr(action, 'tool'):
                        tools_used.append(action.tool)
                    if isinstance(result, dict) and 'results' in result:
                        for search_result in result['results']:
                            if 'content' in search_result and search_result['content']:
                                search_contents.append(search_result['content'].replace("{","(").replace("}",")"))
        else:
            tool_calls_made = False

        logger.info(f"tool_calls_made: {tool_calls_made}, tools_used: {tools_used}")

        # ì›¹ ê²€ìƒ‰ toolì„ ì‚¬ìš©í•œ ê²½ìš° ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦
        if tool_calls_made and  "taviliy_web_search_tool" in tools_used:
            prompt_search_check = ChatPromptTemplate.from_messages(
            [
                ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ [ì›¹ ê²€ìƒ‰ ê²°ê³¼]ë¥¼ ë¹„êµí•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì í•©í•œì§€ íŒë‹¨í•˜ëŠ” ì—­í• ì´ì•¼.
                            [ì¡°ê±´]
                            - ë‹¤ë¥¸ ë¬¸ì¥ ë¶™ì´ì§€ ë§ê³  True ë˜ëŠ” Falseë¡œë§Œ ëŒ€ë‹µí•´.
                            - ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ì í•©í•¨ = True
                            - ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ë¶€ì¡±í•© = False 
                            [ì›¹ ê²€ìƒ‰ ê²°ê³¼]
                            {search_contents[0]}
                            {search_contents[1]}
                            {search_contents[2]}
                            """),
                ("human", "ì‚¬ìš©ì ì§ˆë¬¸: {user_question}, ì›¹ì—ì„œ ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì‚¬ìš©ì ì§ˆë¬¸ì—ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ì í•©í•œì§€ íŒë‹¨í•´."),
            ])
            # .with_structured_output(SearchState)ë¥¼ ì‚¬ìš©í•˜ë©´ ì í•©í•˜ì§€ ì•Šì€ ê²°ê³¼ë¥¼ ì í•©í•˜ë‹¤ê³  íŒë‹¨, ì‚¬ìš©í•˜ì§€ì•Šìœ¼ë©´ ì •í™•í•˜ê²Œ ì˜ íŒë‹¨í•¨
            check_chain = prompt_search_check | llm.with_config({'temperature': 0.2})
            output_check = check_chain.invoke({"user_question": state['messages'][-1].content})
            result = output_check.content.split('</think>\n\n')[-1]

            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ë‹µë³€í•˜ê¸° ì¶©ë¶„í•˜ì—¬ ì¢…ë£Œ
            if result == 'True':
                return Command(
                            goto=END,
                            update={
                            "messages": AIMessage(content=output['output'].split('</think>\n\n')[-1]),
                            "final_answer": output['output'].split('</think>\n\n')[-1],
                            "tools_used": list(set(tools_used)),
                            "tool_calls_made": tool_calls_made,
                            "query_rewrite": False,
                            }
                    )
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì í•©í•˜ì§€ ì•Šì•„ ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±
            else:
                return Command(
                            goto="query_rewrite",
                            update={
                                "query_rewrite": True,
                                "web_results": search_contents
                            }
                        )
        else:
            return Command(
                            goto=END,
                            update={
                            "messages": AIMessage(content=output['output'].split('</think>\n\n')[-1]),
                            "final_answer": output['output'].split('</think>\n\n')[-1],
                            "tools_used": list(set(tools_used)),
                            "tool_calls_made": tool_calls_made
                            }
                    )

    def query_rewrite(state: AppState) -> AppState:
        logger.info(' == [query_rewrite] node init == ')
        web_results = state['web_results']
        prompt_rewrite = ChatPromptTemplate.from_messages(
        [
            ("system", f"""You a question re-writer that converts an input question to a better version that is optimized for web searches.
                        [ì¡°ê±´]
                         - Look at the input and try to reason about the underlying semantic intent / meaning.
                         - ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ê³  í•œêµ­ì–´ë¡œ ë‹µë³€í•´.
                         - [ì´ì „ ê²€ìƒ‰ ê²°ê³¼]ë¥¼ ì°¸ê³ í•´ì„œ, ì´ëŸ° ê²°ê³¼ê°€ ì•ˆë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ìƒì„±í•´.
                        [ì´ì „ ê²€ìƒ‰ ê²°ê³¼]
                         - {web_results[0]}
                         - {web_results[1]}
                         - {web_results[2]}
                        """),
            ("human", "ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸: {user_question}, ì›¹ ê²€ìƒ‰ì„ ìœ„í•´ ê°œì„ ëœ ì§ˆë¬¸ì„ ìƒì„±í•´."),
        ])
        rewrite_chain = prompt_rewrite | llm.with_config({'temperature': 0.5})
        output = rewrite_chain.invoke({"user_question": state['messages'][-1].content})
        return {
                "query": output.content.split('</think>\n\n')[-1],
                }
                    

    def security_filter(state: AppState) -> AppState:
        logger.info(' == [security_filter] node init == ')
        prompt_security = ChatPromptTemplate.from_messages(
        [
            ("system", f"""ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬, ë¯¼ê° ì •ë³´ ì ‘ê·¼ìœ¼ë¡œ ì¸í•œ ë‹µë³€ ë¶ˆê°€ëŠ¥ì„ ì„¤ëª…í•˜ëŠ” ì—­í• ì´ì•¼.
                        [ì¡°ê±´]
                         - ë¯¼ê°í•œ ì •ë³´ì— ëŒ€í•œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬, ê·¸ ì§ˆë¬¸ì— ëŒ€í•´ ì™œ ë‹µë³€í•  ìˆ˜ ì—†ëŠ”ì§€ ì„¤ëª…í•´.
                         - ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ê³  í•œêµ­ì–´ë¡œ ë‹µë³€í•´.
                        """),
            ("human", "{user_question}"),
        ])
        security_chain = prompt_gen | llm.with_config({'temperature': 0.5})
        output = security_chain.invoke({"user_question": state['messages'][-1].content})
        return {
            "messages": AIMessage(content=output.content.split('</think>\n\n')[-1]),
            "final_answer": output.content.split('</think>\n\n')[-1]
            }


    class SQLManager:
        def __init__(self):
            ## db ì—°ê²°
            self.db_user = "admin"
            self.db_password = "sdt251327"
            self.db_host = "127.0.0.1"
            self.db_name = "langgraph" 
            self.connection_string = f"mysql+pymysql://{self.db_user}:{self.db_password}@{self.db_host}/{self.db_name}"
            self.engine = create_engine(self.connection_string)
            self.connection = self.engine.connect()
            self.inspector = inspect(self.engine)
            self.max_gen_sql = 2


        def get_schema(self, state: AppState) -> AppState:
                logger.info(' == [get_schema] node init == ')
                get_schema_result = None
                max_retries = 3
                retry_delay = 1
                for attempt in range(max_retries):
                    try:
                        db_structure= """"""
                        schema_found = False
                        # ìŠ¤í‚¤ë§ˆ ìˆœíšŒ
                        for schema_name in self.inspector.get_schema_names():
                            if schema_name == self.db_name:
                                schema_found = True
                                logger.info(f'ìŠ¤í‚¤ë§ˆ "{schema_name}" ë°œê²¬ë¨')
                                tables = self.inspector.get_table_names(schema=schema_name)
                                for idx, table_name in enumerate(tables):
                                    # ì»¬ëŸ¼ ì´ë¦„ê³¼ íƒ€ì… ìˆ˜ì§‘
                                    columns = self.inspector.get_columns(table_name, schema=schema_name)
                                    column_list = [f"{col['name']}" for col in columns]
                                    db_structure += f'\n[DB ìŠ¤í‚¤ë§ˆ]\n{idx}.table_name: {table_name}\n{idx}.columns: {column_list}'
                                break
                        if not schema_found:
                            raise Exception(f'ìŠ¤í‚¤ë§ˆ "{self.db_name}"ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
                        logger.info('DB ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ')
                        return {
                            "sql_db_schema": db_structure,
                            "sql_error_node": None,
                            "sql_error": None
                        }

                    except Exception as e:
                        logger.error(f'DB ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}')
                        # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë¼ë©´ ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        if attempt < max_retries - 1:
                            time.sleep(retry_delay)
                            logger.info(f'{retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...')
                        else:
                            # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
                            logger.error('ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. DB ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°ë¥¼ í¬ê¸°í•©ë‹ˆë‹¤.')
                            return {
                                "sql_db_schema": None,
                                "sql_error_node": "get_schema",
                                "sql_error": f"get_schema 3íšŒ ì‹œë„ í›„ ì‹¤íŒ¨: {str(e)}"
                            }
                

        def sql_gen_node(self, state: AppState) -> Command[Literal["sql_execute_node", "sql_final_answer_gen"]]:
            logger.info(' == [sql_gen_node] node init == ')
            if state["sql_db_schema"] != None:
                try:
                    # error ë©”ì„¸ì§€ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì„œ ìƒì„±
                    sql_error = state["sql_error"]
                    sql_error_cnt = state["sql_error_cnt"]
                    print(f"sql_error_cnt: {sql_error_cnt}")

                    if sql_error == None:
                        prompt_sql = ChatPromptTemplate.from_messages(
                        [
                            ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì–»ê¸° ìœ„í•´ ì•„ë˜ [DB ìŠ¤í‚¤ë§ˆ]êµ¬ì¡°ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–»ê¸°ìœ„í•œ SQL ë¬¸ì„ ë§Œë“œëŠ” ì—­í• ì´ì•¼. 
                                        ê°€ì¥ ë‹µë³€ì„ ì˜ ì´ëŒì–´ ë‚¼ ìˆ˜ìˆëŠ” SQL ì¡°ê±´ì´ ë¬´ì—‡ì¼ì§€ step by stepìœ¼ë¡œ ì¶©ë¶„íˆ ìƒê°í•´. ì ˆëŒ€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ë¶™ì´ì§€ ë§ê³ , SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´.
                                        [ì¤‘ìš” ì›ì¹™]
                                        - ë¬´ì¡°ê±´ [DB ìŠ¤í‚¤ë§ˆ]ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì•¼ í•¨
                                        - [DB ìŠ¤í‚¤ë§ˆ]ì— ì—†ëŠ” ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•œ ê²½ìš°, ê·¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” SQLì„ ìƒì„±í•´.
                                        - ì ˆëŒ€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ë¶™ì´ì§€ ë§ê³ , SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•¨
                                        - ì—¬ëŸ¬ê°€ì§€ í›„ë³´ë¥¼ ìƒê°í•´ë³´ê³ , ê·¸ ì¤‘ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì˜ ë§ëŠ” ë¬¸ì¥ì„ ì„ íƒí•´

                                        [DB ìŠ¤í‚¤ë§ˆ]
                                        {state['sql_db_schema']}"""),
                            ("human", "{user_question}"),
                        ])
                        sql_chain = prompt_sql | llm.with_config({'temperature': 0, 'timeout': 10})
                        output = sql_chain.invoke({"user_question": state['messages'][-1].content})
                        return Command(
                            goto="sql_execute_node",
                            update={
                                "sql_draft": output.content.split('</think>\n\n')[-1]
                            }
                        )
                    # ì—ëŸ¬ ë°œìƒí•˜ì—¬ ìµœëŒ€ 3ë²ˆ ë‹¤ì‹œ ì‹œë„
                    elif sql_error != None and sql_error_cnt <= self.max_gen_sql:
                        print(f"sql_draft: {state['sql_draft']}")
                        prompt_sql = ChatPromptTemplate.from_messages(
                        [
                            ("system", f"""ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì–»ê¸° ìœ„í•´ ì•„ë˜ [DB ìŠ¤í‚¤ë§ˆ]êµ¬ì¡°ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–»ê¸°ìœ„í•œ SQL ë¬¸ì„ ë§Œë“œëŠ” ì—­í• ì´ì•¼. 
                                        ë„ˆëŠ” ë°©ê¸ˆ ì˜ëª»ëœ SQLì„ ë§Œë“¤ì–´ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´. ì•„ë˜ ì‚¬í•­ë“¤ ì°¸ê³ í•´ì„œ ì˜¬ë°”ë¥¸ SQLë¬¸ì„ ë‹¤ì‹œ ë§Œë“¤ì–´.
                                        ì ˆëŒ€ ë‹¤ë¥¸ ë¬¸ì¥ì„ ë¶™ì´ì§€ ë§ê³ , SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´.
                                        
                                        [ì¤‘ìš”í•œ ì œì•½ì‚¬í•­]
                                        - ì‚¬ìš©ì ì§ˆë¬¸: {state['messages'][-1].content}
                                        - ì´ì „ì— ì‹¤íŒ¨í•œ SQL: {state['sql_draft']}
                                        - ë°œìƒí•œ ì—ëŸ¬: {state['sql_error']}
                                        - ì´ë²ˆì€ {sql_error_cnt}ë²ˆì§¸ ì‹œë„ì…ë‹ˆë‹¤.
                                        - ì ˆëŒ€ ì´ì „ì— ì‹¤íŒ¨í•œ SQLê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , ì™„ì „íˆ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´.
                                        - SQLë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•´.

                                        [DB ìŠ¤í‚¤ë§ˆ]
                                        {state['sql_db_schema']}"""),
                            ("human", "{user_question}"),
                        ])
                        sql_chain = prompt_sql | llm.with_config({'temperature': 0.2 * sql_error_cnt, "timeout": 10})
                        output = sql_chain.invoke({"user_question": state['messages'][-1].content})
                        return Command(
                            goto="sql_execute_node",
                            update={
                                "sql_draft": output.content.split('</think>\n\n')[-1]
                            }
                        )
                    else:
                        return Command(
                            goto="sql_final_answer_gen",
                            update={
                                "sql_draft": "none",
                                "sql_result": "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                            }
                        )

                except Exception as e:
                    logger.info(traceback.format_exc())
                    return {
                        "sql_error_node": "sql_gen_node",
                        "sql_error": e
                    }
            else:
                return Command(
                            goto="sql_final_answer_gen",
                            update={
                                "sql_draft": "none",
                                "sql_result": "í˜„ì¬ ì—°ê²°ì´ ë¶ˆì•ˆì •í•˜ì—¬ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ ë’¤ì— ë‹¤ì‹œ ì§ˆë¬¸í•˜ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”."
                            }
                        )

        def sql_execute_node(self, state: AppState) -> Command[Literal["sql_gen_node", "sql_final_answer_gen"]]:
            logger.info(' == [sql_execute_node] node init == ')
            
            sql_draft = state["sql_draft"]
            try:
                with self.engine.connect() as self.connection:
                    result = self.connection.execute(text(sql_draft))
                    rows = result.fetchall() 
                    # for row in result:
                    #     print(row)
                if not rows or None in rows:
                    raise Exception("EMPTY_RESULT") 
                    
                # ë¹ˆ ê²°ê³¼ë„ ì—ëŸ¬ë¡œ ì²˜ë¦¬ 
                else:
                    return Command(
                    goto="sql_final_answer_gen",
                    update={
                        "sql_result": rows,
                        "sql_error": None,
                        }
                    )

            # ì‹¤í–‰ ì—ëŸ¬ ë°œìƒ ì‹œ sql_gen_node ë…¸ë“œë¡œ ëŒì•„ê°€ë©°, error ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê³  error countë¥¼ ì˜¬ë¦¼.
            except Exception as e:
                return Command(
                    goto="sql_gen_node",
                    update={
                        "sql_error_node": "sql_execute_node",
                        "sql_error": str(e),
                        "sql_error_cnt": state['sql_error_cnt'] + 1
                        }
                    )

        def sql_final_answer_gen(self,state: AppState) -> AppState:
            logger.info(' == [sql_execsql_final_answer_gen] node init == ')

            sql_result = state['sql_result']
            sql_draft = state['sql_draft']
            prompt_final = ChatPromptTemplate.from_messages(
            [
                ("system", f"""ë„ˆëŠ” ì§ˆì˜ì‘ë‹µ ì±—ë´‡ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—­í• ì´ì•¼. 
                            [ì¡°ê±´]
                             - ì•„ë˜ [ì •ë³´]ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ [SQLë¬¸]ìœ¼ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ DBì—ì„œ ì¡°íšŒí•œ ê²°ê³¼ì•¼. ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì„ ë§ë¶™ì´ê±°ë‚˜ ë³€í˜•í•˜ì—¬ í™˜ê°ì„ ì¼ìœ¼í‚¤ì§€ ë§ˆ.
                             - DB ìŠ¤í‚¤ë§ˆ, í…Œì´ë¸” ë“± ë‚´ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ë…¸ì¶œí•˜ì§€ë§ˆ.
                             - í•œêµ­ì–´ë¡œ ë‹µë³€í•´.
                            [SQLë¬¸]\n {sql_draft}
                            [ì •ë³´]\n {sql_result}"""),
                ("human", "{user_question}"),
                MessagesPlaceholder("messages")
            ])
            final_chain = prompt_final | llm.with_config({'temperature': 0})
            output = final_chain.invoke({"user_question": state['messages'][-1].content, "messages": state["messages"]})
            return {
                "messages": AIMessage(content=output.content.split('</think>\n\n')[-1]),
                "final_answer": output.content.split('</think>\n\n')[-1]
            }

    class RAGManager:
        def __init__(self):
            # chromadb ìƒìˆ˜ ì •ì˜
            self.CHROMA_DIR = "./chroma_db"
            self.EMBEDDING_MODEL = "BAAI/bge-m3"  # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
            self.RERANK_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Rerank ëª¨ë¸
            self.COLLECTION = "approval_guide" 
            self.top_k = 3 # í”„ë¡¬í”„íŠ¸ì— ì œê³µí•  ë¬¸ì„œ ê°¯ìˆ˜
            self.embeddings = None
            self.reranker = None

        def rag_init_node(self, state: AppState) -> Command[Literal["rag_execute_node", END]]:
            logger.info(' == [rag_init_node] node init == ')

            try:
                if self.embeddings and self.reranker:
                    pass
                else:
                    self.embeddings = HuggingFaceEmbeddings(
                            model_name=self.EMBEDDING_MODEL,
                            model_kwargs={'device': 'cuda:0'},
                            encode_kwargs={'normalize_embeddings': True}
                        )
                    self.reranker = CrossEncoder(self.RERANK_MODEL)

                # ê¸°ì¡´ DBê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if os.path.exists(self.CHROMA_DIR):
                    self.vectorstore = Chroma(
                        collection_name=self.COLLECTION,
                        persist_directory=self.CHROMA_DIR,
                        embedding_function=self.embeddings
                    )

                    return Command(
                        goto="rag_execute_node",
                        update={
                            "rag_error": None,
                        }
                    )
                else:
                    return Command(
                        goto=END,
                        update={
                            "rag_error": "vectorestore initialize ì‹¤íŒ¨",
                        }
                    )
            except Exception as e:
                print(f"error in initialize_RAG_comp: {e}")
                return Command(
                        goto=END,
                        update={
                            "rag_error": "initialize_RAG_component ì‹¤íŒ¨",
                        }
                    )
                

        def rag_execute_node(self, state: AppState) -> AppState:
            logger.info(' == [rag_execute_node] node init == ')

            # retrive
            self.retriever = self.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
            documents = self.retriever.get_relevant_documents(state['messages'][-1].content)
            # rerank
            query_doc_pairs = [(state['messages'][-1].content, doc.page_content) for doc in documents]
            scores = self.reranker.predict(query_doc_pairs)
            scored_docs = list(zip(scores, documents))
            scored_docs.sort(key=lambda x: x[0], reverse=True)
            # ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜
            reranked_docs = ['<Document>' + doc.page_content[:] + '</Document>' for score, doc in scored_docs[:self.top_k]]

            return {
                "rag_retrieved_docs": documents,
                "rag_reranked_docs": reranked_docs
            }

        def rag_final_answer_gen(self, state: AppState) -> AppState:
            logger.info(' == [rag_final_answer_gen] node init == ')

            rag_reranked_docs = state['rag_reranked_docs']

            prompt_final = ChatPromptTemplate.from_messages(
            [
                ("system", f"""ë„ˆëŠ” ì§ˆì˜ì‘ë‹µ ì±—ë´‡ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—­í• ì´ì•¼. 
                            [ì¡°ê±´]
                             - ì•„ë˜ [ì •ë³´]ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œì—ì„œ ì¡°íšŒí•œ ê²°ê³¼ì•¼. ê²°ê³¼ì— ì—†ëŠ” ë¶€ë¶„ì€ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ê³ , ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì„ ë§ë¶™ì´ê±°ë‚˜ ë³€í˜•í•˜ì—¬ í™˜ê°ì„ ì¼ìœ¼í‚¤ì§€ ë§ˆ. 
                             - í•œêµ­ì–´ë¡œ ë‹µë³€í•´.
                            [ì •ë³´] \n {rag_reranked_docs}"""),
                ("human", "{user_question}"),
                MessagesPlaceholder("messages")
            ])
            final_chain = prompt_final | llm.with_config({'temperature': 0})
            output = final_chain.invoke({"user_question": state['messages'][-1].content, "messages": state["messages"]})
            return {
                "messages": AIMessage(content=output.content.split('</think>\n\n')[-1]),
                "final_answer": output.content.split('</think>\n\n')[-1]
            }

        def hallucination_check(self, state: AppState) -> AppState:
            logger.info(' == [hallucination_check] node init == ')

            rag_reranked_docs = state['rag_reranked_docs']
            final_answer = state['final_answer']

            prompt_check = ChatPromptTemplate.from_messages(
            [
                ("system", f"""ë„ˆëŠ” RAG(Retrieval-Augmented Generation)ê²°ê³¼ë¬¼ì¸ [ë¬¸ì„œ ì •ë³´]ê³¼ LLM ëª¨ë¸ì´ ìƒì„±í•œ [ìƒì„± ë‹µë³€]ì„ ë¹„êµí•˜ì—¬, [ìƒì„± ë‹µë³€]ì— [ë¬¸ì„œ ì •ë³´]ì— ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ìˆëŠ”ì§€ hallucination ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ì—­í• ì´ì•¼.
                            \n[ì¡°ê±´]\n : hallucination ë°œìƒ ì‹œ True, ì—†ì„ ì‹œ Falseë¥¼ 'result' keyì— ë°˜í™˜í•˜ì„¸ìš”. ê·¸ë¦¬ê³  hallucinationì´ ë°œìƒí–ˆë‹¤ê³  íŒë‹¨í•œ ì´ìœ ë¥¼ 'reason' keyì— ë°˜í™˜í•˜ì„¸ìš”.
                            \n[ë¬¸ì„œ ì •ë³´]\n {rag_reranked_docs}
                            \n[ìƒì„± ë‹µë³€]\n {final_answer}
                            """),
                ("human", "{user_question}")
            ])
            final_chain = prompt_check | llm.with_config({'temperature': 0}).with_structured_output(HallucinationState)
            output = final_chain.invoke({"user_question": state['messages'][-1].content})
            check_result = output['result']

            # hallucination ë°œìƒ
            if check_result:
                return {
                            "hallucination_check": output,
                            "final_answer": state["final_answer"] + "\n* ì´ ë‹µë³€ì€ ì •í™•í•˜ì§€ ì•Šì€ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ì  ì°¸ê³ ë°”ëë‹ˆë‹¤.\n"
                        }
            # hallucination ë°œìƒí•˜ì§€ ì•Šì•„ì„œ ì¢…ë£Œ
            else:
                return {
                            "hallucination_check": output,
                        }

    rag_manager = RAGManager()
    sql_manager = SQLManager()

    builder = StateGraph(AppState)
    # node
    builder.add_node("router", router_node)
    builder.add_node("general", general)
    builder.add_node("query_rewrite", query_rewrite)
    builder.add_node("security_filter", security_filter)
    builder.add_node("get_schema", sql_manager.get_schema)
    builder.add_node("sql_gen_node", sql_manager.sql_gen_node)
    builder.add_node("sql_execute_node", sql_manager.sql_execute_node)
    builder.add_node("sql_final_answer_gen", sql_manager.sql_final_answer_gen)
    builder.add_node("rag_init_node", rag_manager.rag_init_node)
    builder.add_node("rag_execute_node", rag_manager.rag_execute_node)
    builder.add_node("rag_final_answer_gen", rag_manager.rag_final_answer_gen)
    builder.add_node("hallucination_check", rag_manager.hallucination_check)

    # edge
    builder.add_edge(START, "router")
    builder.add_edge("get_schema", "sql_gen_node")
    builder.add_edge("sql_final_answer_gen", END)
    builder.add_edge("query_rewrite", "general")
    builder.add_edge("rag_execute_node", "rag_final_answer_gen")
    builder.add_edge("rag_final_answer_gen", "hallucination_check")
    builder.add_edge("hallucination_check", END)
    builder.add_edge("security_filter", END)

    checkpointer = MemorySaver()
    store = InMemoryStore()
    #compile
    graph = builder.compile(checkpointer=checkpointer)

    # # visualization
    # from IPython.display import Image
    # # PNG ë°”ì´íŠ¸ ìƒì„±
    # png_bytes = graph.get_graph().draw_mermaid_png()
    # # íŒŒì¼ë¡œ ì €ì¥
    # with open("graph.png", "wb") as f:
    #     f.write(png_bytes)

    return graph


# ê·¸ë˜í”„ ì´ˆê¸°í™”
try:
    graph = initialize_graph()
    st.success("âœ… ì±—ë´‡ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
except Exception as e:
    st.error(f"âŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    st.info("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.write(message["content"])


if "execution_count" not in st.session_state:
    st.session_state.execution_count = 0

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    #logger.info(f"ì‚¬ìš©ì ì…ë ¥ : {prompt}")
    timestamp = time.time()
    st.session_state.execution_count += 1
    logger.info(f"[ì„¸ì…˜ID: {st.session_state.thread_id}] - [{st.session_state.execution_count}ë²ˆì§¸ ì‹¤í–‰] ì‹¤í–‰ ì‹œì : {timestamp} - ì‚¬ìš©ì ì…ë ¥: {prompt}")
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.write(prompt)
    
    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                # LangGraphì— ë©”ì‹œì§€ ì „ì†¡
                #human_message = HumanMessage(content=prompt)
                human_message = {
                    "messages": [{"role":"user","content":prompt}], 
                    "sql_draft": "",
                    "sql_result": "",
                    "sql_error": None,
                    "sql_error_cnt": 0,
                    "sql_error_node": "none",
                    "rag_check_cnt":0,
                    "rag_retrieved_docs": [],
                    "rag_reranked_docs": [],
                    "rag_error": None,
                    }
                config = {"configurable": {"thread_id": st.session_state.thread_id}}
                
                # ê·¸ë˜í”„ ì‹¤í–‰
                result = graph.invoke(
                    human_message, 
                    config=config
                )
                logger.info(f"AppState : {result}")
                
                # ê²°ê³¼ì—ì„œ ë‹µë³€ ì¶”ì¶œ
                if "final_answer" in result and result["final_answer"]:
                    response = result["final_answer"]
                elif result["messages"]:
                    response = result["messages"][-1].content
                else:
                    response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                st.write(response)
                
                # ì‘ë‹µì„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                logger.info(f"AI ë‹µë³€ : {response}")
                
            except Exception as e:
                error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_message)
                st.session_state.chat_history.append({"role": "assistant", "content": error_message})

# ì‚¬ì´ë“œë°”ì— ì¶”ê°€ ì •ë³´
with st.sidebar:
    st.header("â„¹ï¸ ì •ë³´")
    st.markdown("""
    **ì‚¬ìš© ê°€ëŠ¥í•œ ì§ˆë¬¸ ìœ í˜•:**
    - ğŸ“‹ ì‚¬ë‚´ ê²°ì¬ ê·œì • ê´€ë ¨ ì§ˆë¬¸
    - ğŸ‘¥ ì‚¬ë‚´ ì§ì› ì •ë³´ ê´€ë ¨ ì§ˆë¬¸
    - ğŸ’¬ ê¸°íƒ€ ì¼ë°˜ì ì¸ ì§ˆë¬¸
    
    **ì‚¬ìš©ë²•:**
    1. ì•„ë˜ ì…ë ¥ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
    2. ì±—ë´‡ì´ ìë™ìœ¼ë¡œ ì§ˆë¬¸ ìœ í˜•ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤
    3. ì ì ˆí•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤
    """)
    
    if st.button("ğŸ”„ ì±„íŒ… ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("**ì—°ê²° ìƒíƒœ:**")
    if "graph" in locals():
        st.success("âœ… ì—°ê²°ë¨")
    else:
        st.error("âŒ ì—°ê²° ì•ˆë¨")
